{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ea5659",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Algorithm\n",
    "\n",
    "- Algorithm : TD3\n",
    "- Image \n",
    " - shape : 84 x 84 x 4\n",
    " - style : skipping\n",
    "- Output \n",
    " - shape : 2 dimension \n",
    " - style : r, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d42c1",
   "metadata": {},
   "source": [
    "#### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4c9794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.738359Z",
     "start_time": "2022-05-09T09:33:24.103562Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00dca46c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.741072Z",
     "start_time": "2022-05-09T09:33:24.739317Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/lim/Artificial General Intelligence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f32e38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.807446Z",
     "start_time": "2022-05-09T09:33:24.741796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from sys_util import makedir\n",
    "from itertools import count\n",
    "from game import RL_game, plot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd228c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.845403Z",
     "start_time": "2022-05-09T09:33:24.808765Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Device Name : cuda\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('=' * 50)\n",
    "print('Device Name : {}'.format(device))\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906b199",
   "metadata": {},
   "source": [
    "#### Replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f616f435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.873563Z",
     "start_time": "2022-05-09T09:33:24.846146Z"
    }
   },
   "outputs": [],
   "source": [
    "class Replay_buffer:\n",
    "    def __init__(self, max_size = 50000):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] == data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        index = np.random.randint(0, len(self.storage), size = batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "        \n",
    "        for i in index:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "            \n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafc373",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45a4a3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.877628Z",
     "start_time": "2022-05-09T09:33:24.874286Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, frame_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 4, out_channels = 32, kernel_size = 8, stride = 4, padding = 'valid')\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2, padding = 'valid')\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 'valid')\n",
    "        self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 512, kernel_size = 7, stride = 1, padding = 'valid')\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((-1, 4, self.frame_size, self.frame_size))\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c328e4",
   "metadata": {},
   "source": [
    "#### Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc60776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.881190Z",
     "start_time": "2022-05-09T09:33:24.878296Z"
    }
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, model):\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv = model\n",
    "        \n",
    "        self.fc1 = nn.Linear(s_dim, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.action_r = nn.Linear(100, 1)\n",
    "        self.action_theta = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        action_r = torch.sigmoid(self.action_r(x))\n",
    "        action_theta = torch.tanh(self.action_theta(x))\n",
    "        \n",
    "        action = torch.cat([action_r, action_theta], dim=1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaa293",
   "metadata": {},
   "source": [
    "#### Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa9c097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.885532Z",
     "start_time": "2022-05-09T09:33:24.881882Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, model):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv = model\n",
    "        \n",
    "        self.fc1 = nn.Linear(s_dim + a_dim, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "        \n",
    "        self.fc4 = nn.Linear(s_dim + a_dim, 200)\n",
    "        self.fc5 = nn.Linear(200, 100)\n",
    "        self.fc6 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x, u):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x1 = F.relu(self.fc1(torch.cat([x, u], dim = 1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        x1 = self.fc3(x1)\n",
    "        \n",
    "        x2 = F.relu(self.fc4(torch.cat([x, u], dim = 1)))\n",
    "        x2 = F.relu(self.fc5(x2))\n",
    "        x2 = self.fc3(x2)\n",
    "        return x1, x2\n",
    "    \n",
    "    def Q1(self, x, u):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x1 = F.relu(self.fc1(torch.cat([x, u], dim = 1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        x1 = self.fc3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b94d43",
   "metadata": {},
   "source": [
    "#### RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d361ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.896224Z",
     "start_time": "2022-05-09T09:33:24.886213Z"
    }
   },
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    def __init__(self, state_dim, action_dim, action_lr, critic_lr, model1, model2):\n",
    "        self.actor = Actor(state_dim, model1).to(device)\n",
    "        self.actor_target = Actor(state_dim, model1).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = action_lr)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim, model2).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim, model2).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = critic_lr)\n",
    "        \n",
    "        self.total_it = 0\n",
    "        self.replay_buffer = Replay_buffer()\n",
    "        \n",
    "    def select_action(self, state, noise = 0.1):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action_r, action_theta = self.actor(state).cpu().data.numpy().flatten()\n",
    "        \n",
    "        if noise != 0:\n",
    "            action_r = (action_r + (np.random.normal(0, noise, size=1))).clip(0, 1)\n",
    "            action_theta = (action_theta + (np.random.normal(0, noise * 2, size=1))).clip(-1, 1)\n",
    "        \n",
    "        action_r = torch.from_numpy(action_r).reshape(-1, 1)\n",
    "        action_theta = torch.from_numpy(action_theta).reshape(-1, 1)\n",
    "        \n",
    "        action = torch.cat([action_r, action_theta], dim = 1)\n",
    "        return action.cpu().data.numpy().flatten()\n",
    "    \n",
    "    def update(self, batch_size, iterations, discount = 0.99, tau = 0.005, policy_noise = 0.2, noise_clip = 0.5, policy_freq = 2):\n",
    "        for it in range(iterations):\n",
    "            x, y, u, r, d = self.replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            done = torch.FloatTensor(1-d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_action = self.actor_target(next_state)\n",
    "                \n",
    "                r_noise = (torch.randn(size=(batch_size,)) * policy_noise / 2).clamp(-noise_clip/4, noise_clip/4).to(device)\n",
    "                theta_noise = (torch.randn(size=(batch_size,)) * policy_noise).clamp(-noise_clip/2, noise_clip/2).to(device)\n",
    "                \n",
    "                next_action[:, 0] += r_noise\n",
    "                next_action[:, 1] += theta_noise\n",
    "                \n",
    "                next_action[:, 0] = next_action[:, 0].clamp(0, 1)\n",
    "                next_action[:, 1] = next_action[:, 1].clamp(-1, 1)\n",
    "                \n",
    "                target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "                target_Q = torch.min(target_Q1, target_Q2)\n",
    "                target_Q = reward + (done * discount * target_Q).detach()\n",
    "                \n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "            \n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            if it % policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    def save(self, directory, epoch):\n",
    "        torch.save(self.actor, directory + '/Actor' + '/actor_{}.pt'.format(epoch))\n",
    "        torch.save(self.critic, directory + '/Critic' + '/critic_{}.pt'.format(epoch))\n",
    "        \n",
    "        print('')\n",
    "        print('=' * 50)\n",
    "        print('Epoch : {} // Model has been saved...'.format(epoch))\n",
    "        print('=' * 50)\n",
    "    \n",
    "    def load(self, directory, epoch, device):\n",
    "        self.actor = torch.load(directory + '/Actor' + '/actor_{}.pt'.format(epoch), map_location=torch.device(device))\n",
    "        self.critic = torch.load(directory + '/Critic' + '/critic_{}.pt'.format(epoch), map_location=torch.device(device))\n",
    "        \n",
    "        print('')\n",
    "        print('=' * 50)\n",
    "        print('Model has been loaded...')\n",
    "        print('=' * 50)\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f7b5b",
   "metadata": {},
   "source": [
    "#### parameter import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf4f9b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:24.930677Z",
     "start_time": "2022-05-09T09:33:24.897800Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lim/Artificial General Intelligence/game.py:127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.action_r = gym.spaces.Box(low=np.float(0), high=np.float(act_high), shape=(1,))\n"
     ]
    }
   ],
   "source": [
    "env = RL_game(mode = 'base')\n",
    "frame_size = 84\n",
    "state_dim = 256\n",
    "action_dim = 2\n",
    "action_lr = 1e-5\n",
    "critic_lr = 1e-5\n",
    "batch_size = 128\n",
    "tau = 0.01\n",
    "model1 = ConvNet(frame_size)\n",
    "model2 = ConvNet(frame_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9353b4c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:31.370590Z",
     "start_time": "2022-05-09T09:33:24.931343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name : TD3_frame_skip\n",
      "==================================================\n",
      "Directory has been made...\n",
      "Directory path : /home/lim/Artificial General Intelligence/Data/RL_model/TD3_frame_skip\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_episode = 100000\n",
    "start_train = 100\n",
    "trial = input('Name : ')\n",
    "directory = makedir('/home/lim/Artificial General Intelligence/Data/RL_model', trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f261d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:31.373899Z",
     "start_time": "2022-05-09T09:33:31.371292Z"
    }
   },
   "outputs": [],
   "source": [
    "path = np.load('/home/lim/Artificial General Intelligence/Data/total_path.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccf5255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T09:33:31.504486Z",
     "start_time": "2022-05-09T09:33:31.374783Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f67831c7880>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQd0lEQVR4nO3db2xd9X3H8ffX959j31wcEzdxguOAgmjKtJJglTIqwUpbkQxBmbStSJXYtIknHaPTJhq2BxOTKiFtqsqDaRKi7dDatWWUFsSDtiEr2yoEAwPJSAIE2iwk3CT+A7avY1//++7BPXGtkGsf+95r35Pf5yVZ1+f4Xp/zI3z8O+fcc79fc3dE5NLXstY7ICKrQ2EXCYTCLhIIhV0kEAq7SCAUdpFA1BR2M7vNzN4ys3fMbF+9dkpE6s9W+j67maWAt4HPAyeBl4G73f1I/XZPROolXcNrPwW84+6/AjCzHwB3AlXDbma6g0ekwdzdLra+lsP4rcB7C5ZPRutEpAnVMrNf7K/HR2ZuM7sXuLeG7YhIHdQS9pNAz4LlK4D3L3ySuz8KPAo6jBdZS7Ucxr8MXG1mV5pZFvgS8Ex9dktE6m3FM7u7z5jZnwM/A1LAt939cN32TETqasVvva1oYzqMF2m4RlyNF5EEUdhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBWDLsZvZtMztrZm8sWNdpZvvN7Fj0uKGxuykitYozs/8LcNsF6/YBB9z9auBAtCwiTWzJsLv7fwHDF6y+E3g8+v5x4Iv13S0RqbeVnrNvcvciQPT4sfrtkog0Qi1NImJRRxiR5rDSmf2MmXUDRI9nqz3R3R919z5371vhtkSkDlYa9meAe6Lv7wGers/uiEijLNkkwsy+D9wCbATOAH8H/AR4AtgGnAD+wN0vvIh3sd+lJhEiDVatSYQ6wohcYtQRRiRwCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQcTrC9JjZL8zsqJkdNrP7o/XqCiOSIHFq0HUD3e7+qpmtB/qpNIX4Y2DY3R82s33ABnf/2hK/S2WpRBpsxWWp3L3o7q9G348BR4GtqCuMSKIsq0mEmW0HdgEvcUFXGDO7aFcYNYkQaQ6xq8uaWR74T+Dr7v6UmX3o7h0Lfv6Buy963q7DeJHGq6m6rJllgB8B33P3p6LVsbvCiMjai3M13oBvAUfd/RsLfqSuMCIJEudq/GeA/wb+F5iLVv8NlfP2ZXWF0WG8SOOpI4xIINQRRiRwCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQcWrQtZrZ/5jZwagjzEPRenWEEUmQODXoDGh391JUZfaXwP3A76OOMCJNp5aOMO7upWgxE3056ggjkihx68anzOx1KrXh97v7RzrCAFU7wpjZK2b2Sp32WURWYFnVZc2sA/gxcB/wS3WEEWk+daku6+4fAs8Dt6GOMCKJEudqfFc0o2Nm64DPAW+ijjAiiRLnavxvU7kAl6Lyx+EJd/97M7scdYQRaTrqCCMSCHWEEQmcwi4SCIVdJBAKu0ggFHaRQCjsIoFQ2EUCobCLBEJhFwmEwi4SCIVdJBAKu0ggFHaRQCjsIoFQ2EUCobCLBEJhFwlE7LBH5aRfM7Nno2V1hBFJkOXM7PcDRxcs7wMOuPvVwIFoWUSaVNwmEVcAvwc8tmC1OsKIJEjcmf2bwAPA3IJ16ggjkiBx6sbfDpx19/6VbMDdH3X3PnfvW8nrRaQ+0jGecxNwh5ntBVqBgpl9l6gjjLsX1RFGpPktt9fbLcBfu/vtZvYPwNCCls2d7v7AEq9X3fiEamtrI5/Pk8lkWL9+Pa2traRSKVKpFADlcplyuczk5CTDw8OUy2VmZ2eZmZlZ4z0PT7W68XFm9moeBp4wsz8l6ghTw++SJmZmbNmyhWuvvZaOjg76+vro6ekhl8vR3t6Ou1MsFhkYGODUqVM899xzFItFSqUSo6OjrGYjEqluWWF39+epNHbE3YeAW+u/S9JMzIyWlhby+TybNm2iq6uLnTt3smPHDtatW0ehUMDdOX78OMVikVwux4YNGxgdHWVqagozU9ibRC0zu1zi2tra2LJlC/l8nhtuuIGbb76Zjo4Otm3bRqFQIJPJkE6ncXc6OzvJZDJks1n27NnDwMAA/f39vPjii5TLZaanp5mbm1t6o9IwCrtU1d7ezrXXXsumTZu4+eab2bt3L62traTTaVpaKm/kmFVOD7u6uti4cSObN2+mt7eX8fFxMpkMR44coVQqMTc3p7CvMYVdqspkMnR0dNDV1cVll11Ga2sruVzuos89H/5cLkc+nyeVSlEoFMjn87j7/OweZ5u5XI65ubn5i3xSHwq7VFUoFOjr62Pnzp1s27aNdHrp/13S6TTt7e1kMhl6e3u5/vrrGRwc5ODBg0xMTCz6WjOju7ub7du3Uy6XOXbsGMPDi3YBl2XQp96kqtbWVnp6etixYwcbN26cn70XY2Zks1na2tro7Oxk69atdHd309raGuu1hUKBrVu3snnz5livkfg0s0tVLS0t5HI51q1bRyaTWfbr0+k069atI5fLzb8fX2072WyWTCZDT08Pu3fvZnx8nFKpRFtbG2NjYwwODuqQvkYKu1TV0tJCe3s7hUKBVCo1fzEurmw2y/r16ymVSov+sUin0xQKBdra2ti9ezd33XXX/Gvefvtt3n33XUZHR5c8DZDF6TBeFuXuNb1P7u7Mzc0t+jvOv5ff0tJCKpUinU6TSqXIZDLzb+9J7fRfUaqampqiWCxy/PhxOjs76erqinXeDpWQj4yMcOLECc6ePbvorDwzM8Po6CiTk5O88MILzM3NMTk5ycGDBykWi4yMjMS6ki+LU9ilqqmpKQYGBigWi6TTaTZu3Lis15dKJU6fPs3g4CCTk5NVnzc7O8u5c+eYmJjg0KFDnD59munpaYrFImNjY7oDr04UdqlqcnKSU6dOkcvlyGazdHd3k8vlSKfTVc/fp6enGRsb49y5cwwMDDA4OBh7ZnZ3pqamKJVKzMzMMD09raDXkcIuVQ0PD7N//342bNjAnj176O3tpb29nXw+TzabvehrRkdHOXToEIODg/T393P48GHOnTu36My+UKlUYnJyEnfXoXudKexSVblc5vTp04yNjTEwMMD4+DipVIpsNvuRi2bnZ+CJiQmGhoY4c+YMQ0NDjIyMUC6XY29zZmZGH4ttEIVdqpqdnaVUKjE1NUV/fz/ZbJZCoUBvby+dnZ2k0+n5GX5kZIRSqTT/AZihoSGOHTum4DaRZRWvqHljKl6ROGaGmZHP5+fvdd+1axc9PT20trbOf8T1xIkTFItFhoaGOHz4MB9++CGzs7O6EWYNNKJ4hQTg/Pvs5XKZUqmEuzM8PEwulyOXyzE+Ps7c3Bxnz56dP2yfmJhgampqrXddLqCZXWJZeMPL+U/Anb/xxd2ZmJhgcnJy/mq8Dt/XTrWZPVbYzew4MAbMAjPu3mdmncAPge3AceAP3f2DJX6Pwi7SYNXCvpzbZX/X3a9bUBJaHWFEEqSWe+PVEUYkQeKG3YGfm1m/md0brVNHGJEEiXvOvsXd3zezjwH7gfuAZ9y9Y8FzPnD3RTu56pxdpPFqOmd39/ejx7PAj4FPEXWEAVBHGJHmF6fXW7uZrT//PfAF4A3gGeCe6Gn3AE83aidFpHZLHsab2VVUZnOo3ITzb+7+dTO7HHgC2EbUEcbdF60OqMN4kcar6X32elHYRRqvHu+zi0iCKewigVDYRQKhsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFAKOwigVDYRQKhsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFAxAq7mXWY2ZNm9qaZHTWzG82s08z2m9mx6HHRyrIisrbizuyPAD91948DnwSOoo4wIokSp+BkATgIXOULnmxmbwG3uHsxKiX9vLtfs8TvUg06kQarpQbdVcAA8B0ze83MHotKSqsjjEiCxJnZ+4AXgZvc/SUzewQYBe5TRxiR5lPLzH4SOOnuL0XLTwK7UUcYkURZMuzufhp4z8zOn4/fChxBHWFEEiVuY8frgMeALPAr4E+o/KFQRxiRJqOOMCKBUEcYkcAp7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIJYMu5ldY2avL/gaNbOvqkmESLIsq1KNmaWAU8ANwFeAYXd/2Mz2ARvc/WtLvF6VakQarF6Vam4F3nX3/wPuBB6P1j8OfHHFeyciDbfcsH8J+H70fawmESLSHGKH3cyywB3Avy9nA+oII9IcljOz7wFedfcz0XKsJhHu/qi797l7X227KiK1WE7Y7+Y3h/CgJhEiiRK3SUQb8B6VTq4j0brLUZMIkaajJhEigVCTCJHAKewigVDYRQKhsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFAKOwigVDYRQKhsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFAxAq7mf2lmR02szfM7Ptm1qqOMCLJEqf901bgL4A+d/8tIEWlfvw+4IC7Xw0ciJZFpEnFPYxPA+vMLA20Ae+jjjAiibJk2N39FPCPVCrIFoERd/856ggjkihxDuM3UJnFrwS2AO1m9uW4G1BHGJHmEOcw/nPAr919wN2ngaeA30EdYUQSJU7YTwCfNrM2MzMqnVyPoo4wIokStyPMQ8AfATPAa8CfAXnUEUak6agjjEgg1BFGJHAKu0ggFHaRQCjsIoFIr/L2BoHx6PFSsRGNp5ldSuOJM5beaj9Y1avxAGb2yqV0g43G09wupfHUOhYdxosEQmEXCcRahP3RNdhmI2k8ze1SGk9NY1n1c3YRWRs6jBcJxKqG3cxuM7O3zOwdM0tUGSsz6zGzX5jZ0age3/3R+kTX4jOzlJm9ZmbPRsuJHY+ZdZjZk2b2ZvTvdGPCx1PX2o+rFnYzSwH/BOwBPgHcbWafWK3t18EM8FfuvhP4NPCVaP+TXovvfiofWT4vyeN5BPipu38c+CSVcSVyPA2p/ejuq/IF3Aj8bMHyg8CDq7X9BoznaeDzwFtAd7SuG3hrrfdtGWO4Ivof5rPAs9G6RI4HKAC/JroOtWB9UsezFXgP6KRy89uzwBdqGc9qHsaf3/nzTkbrEsfMtgO7gJdIdi2+bwIPAHML1iV1PFcBA8B3otOSx8ysnYSOxxtQ+3E1w36xz9gm7q0AM8sDPwK+6u6ja70/K2VmtwNn3b1/rfelTtLAbuCf3X0XlduyE3HIfjG11n68mNUM+0mgZ8HyFVRKUieGmWWoBP177v5UtDpWLb4mdBNwh5kdB34AfNbMvktyx3MSOOnuL0XLT1IJf1LHU1Ptx4tZzbC/DFxtZleaWZbKxYZnVnH7NYnq730LOOru31jwo0TW4nP3B939CnffTuXf4j/c/cskdzyngffM7Jpo1a3AERI6HhpR+3GVLzrsBd4G3gX+dq0vgixz3z9D5bTjEPB69LUXuJzKRa5j0WPnWu/rCsZ2C7+5QJfY8QDXAa9E/0Y/ATYkfDwPAW8CbwD/CuRqGY/uoBMJhO6gEwmEwi4SCIVdJBAKu0ggFHaRQCjsIoFQ2EUCobCLBOL/AYZsVvmwK8XjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(np.array([15]), np.array([0]), path)\n",
    "print(env.action)\n",
    "plt.imshow(env.to_frame(84, 84), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d1621",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b7c72",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-09T09:33:24.121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 100, Total_step : 88, Total Reward : 31.037\n",
      "==================================================\n",
      "***** Now Train begins. *****\n",
      "==================================================\n",
      "Episode : 108, Total_step : 236, Total Reward : 130.44\n",
      "==================================================\n",
      "Epoch : 108 // Model has been saved...\n",
      "==================================================\n",
      "Episode : 111, Total_step : 248, Total Reward : 142.30\n",
      "==================================================\n",
      "Epoch : 111 // Model has been saved...\n",
      "==================================================\n",
      "Episode : 185, Total_step : 416, Total Reward : 89.795"
     ]
    }
   ],
   "source": [
    "agent = TD3(state_dim, action_dim, action_lr, critic_lr, model1, model2)\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(max_episode):\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    env.reset()\n",
    "    state = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        state.append(env.to_frame(frame_size, frame_size).squeeze().copy() / 255)\n",
    "    state = np.array(state)\n",
    "    \n",
    "    for t in count():\n",
    "        reward = 0\n",
    "        \n",
    "        if episode < start_train:\n",
    "            action_r = (np.random.normal(0, 0.2, size=1)).clip(0, 1)\n",
    "            action_theta = np.random.normal(0, 0.4, size=1).clip(-1, 1)\n",
    "        else:\n",
    "            action_r, action_theta = agent.select_action(state, noise = 0.1)\n",
    "            action_r = np.array([action_r])\n",
    "            action_theta = np.array([action_theta])\n",
    "        \n",
    "        next_state = []\n",
    "        \n",
    "        for _ in range(4):\n",
    "            next_tmp, reward_tmp, done, _ = env.step(action_r, action_theta, path)\n",
    "            next_tmp = env.to_frame(frame_size, frame_size).squeeze().copy() / 255\n",
    "            next_state.append(next_tmp)\n",
    "            reward += reward_tmp\n",
    "        \n",
    "        next_state = np.array(next_state)\n",
    "        \n",
    "        action = np.array([action_r.item(), action_theta.item()], dtype = float)\n",
    "        agent.replay_buffer.push((state, next_state, action, reward, float(done)))\n",
    "        state = next_state.copy()\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print('\\rEpisode : {}, Total_step : {}, Total Reward : {:.2f}'.format(episode, env.count, total_reward), end='')\n",
    "    if episode == start_train:\n",
    "        print('')\n",
    "        print('=' * 50)\n",
    "        print('***** Now Train begins. *****')\n",
    "        print('=' * 50)\n",
    "        \n",
    "    reward_list.append(total_reward)\n",
    "    \n",
    "    if episode > start_train and total_reward == max(reward_list):\n",
    "        agent.save(directory = directory, epoch = episode)\n",
    "    \n",
    "    if episode > start_train:\n",
    "        agent.update(batch_size, episode)\n",
    "    \n",
    "    if total_reward > 29000:\n",
    "        np.save(directory + '/{}_lr.npy'.format(trial), reward_list)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05c984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
